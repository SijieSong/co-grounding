<!DOCTYPE HTML>
<html class="no-js">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    <title>Co-grounding</title>
    <meta name="description" content="Lithium Description" />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">

    <link href="css/plugins.css" media="screen" rel="stylesheet" type="text/css" />
    <link href="css/application.css" media="screen" rel="stylesheet" type="text/css" />
  </head>

<body>

    <!-- ABOUT -->

    <section id="page-about" class="section">
      <div align="center" style="padding-bottom: 100px;">
        <p class="copy-02">CVPR 2021</p>
        <p class="heading h-01">Co-Grounding Networks with Semantic Attention <br> for Refering Expression Comprehension in Videos</p>

        <p class="copy-02">
          <a href="https://sijiesong.github.io/">Sijie Song</a> &nbsp;&nbsp;&nbsp;
          <a href="https://xudonglinthu.github.io/">Xudong Lin</a> &nbsp;&nbsp;&nbsp;
          <a href="https://www.icst.pku.edu.cn/struct/people/liujiaying.html">Jiaying Liu</a> &nbsp;&nbsp;&nbsp;
          <a href="https://www.icst.pku.edu.cn/english/people/gg/1297382.htm">Zongming Guo</a> &nbsp;&nbsp;&nbsp;
          <a href="http://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a> &nbsp;&nbsp;&nbsp;
        </p>
      </div>

<div class="site-inner">
        <h3 class="heading h-03">Abstract</h3>
       
          
            <p class="copy-02">In this paper, we address the problem of referring expression comprehension in videos, which is challenging due to complex expression and scene dynamics. Unlike previous methods which solve the problem in multiple stages (i.e., tracking, proposal-based matching), we tackle the problem from a novel perspective, \textbf{co-grounding}, with an elegant one-stage framework. We enhance the single-frame grounding accuracy by semantic attention learning and improve the cross-frame grounding consistency with co-grounding feature learning. Semantic attention learning explicitly parses referring cues in different attributes to reduce the ambiguity in the complex expression. Co-grounding feature learning boosts visual feature representations by integrating temporal correlation to reduce the ambiguity caused by scene dynamics. Experiment results demonstrate the superiority of our framework on the video grounding datasets VID and LiOTB in generating accurate and stable results across frames. Our model is also applicable to referring expression comprehension in images, illustrated by the improved performance on the RefCOCO dataset.</p>
          
         
            <div align="center" style="padding-top: 50px;padding-bottom:10px">
              <img src="teaser.png" width=70%> <br>
            </div>
            <p class='copy-02' style="color:#888888">Figure 1. Referring expression comprehension in videos. Due to dynamic scenes and ambiguity in the expression, per-frame inference (in blue) with state-of-the-art grounding method would lead to unstable results across frames, while our co-grounding networks achieve accurate and consistent predictions (in red). Ground-truth annotations are denoted in green.
            </p>
         
        </div>
      </div>



      <div class="site-inner" style="padding-top:50px;">
        <p class="heading h-03"> Resources </p> 
          <ul style="line-height:1.5; padding-left: 50px; padding-right: 50px">
          　　<li class="copy-02"> Paper: <a href="https://arxiv.org/abs/2103.12346">arXiv</a></li>
          </ul>
      </div>
      

      <div class="site-inner" style="padding-top:50px;">
        <p class='heading h-03'> Citation</p>
        <p class="copy-02"> @InProceedings{cogrounding_2021_CVPR, <br>
        &nbsp; &nbsp; author = {Song, Sijie and Lin, Xudong and Liu, Jiaying and Guo, Zongming and Chang, Shih-Fu }, <br>
        &nbsp; &nbsp; title = {Co-Grounding Networks with Semantic Attention for Referring Expression Comprehension in Videos}, <br>
        &nbsp; &nbsp; booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, <br>
        &nbsp; &nbsp; month = {June}, <br>
        &nbsp; &nbsp; year = {2021} <br>
        } <br> 
        </p>
      </div>
      
    <section id="page-about" class="section">

</body>
</html>